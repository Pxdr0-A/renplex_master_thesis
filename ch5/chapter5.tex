\chapter{Conclusions \& Future Work}

This Master Thesis delivers an open-source library for modulating Complex-Valued Neural Networks, with an overview on its inner-workings, how to interact with the library and a few simple case studies. It was provided a contextualization in Chapter~\ref{chap:Chapter1} giving a reason why these types of neural networks are relevant to be explored and its potential. Then, in Chapter~\ref{sec:chap1_sota}, a brief exploration of the State-of-the-art to see what are the most common functionalities, components and ways to modulate a CVNN, as well as, some major application where CVNN have a promising impact. It was also performed a search on already existent libraries that modulate CVNN and concluded that is still an incomplete field in need of tools for exploration of these networks. In Chapter~\ref{chap:Chapter3}, was established the mathematical foundation necessary for CVNN moduling together with some core algorithms and procedures developed. Lastly, the library was evaluated in Chapter~\ref{cp:eval} against the MNIST and a Synthetic Signal Reconstruction datasets, with multiple architectures namely Multi-Layer Perceptron, Auto-Encoder and Convolutional Neural Networks.

Despite being one author involved in the development of this library, this Master Thesis, pinpoints a solid start for CVNN exploration at a small scale with scaling capabilities. The reason being is the fact that the library was scaffold using a fast and secure system's programing language (Rust \parencite{rust2018}) and easy interaction at the development level for improving core algorithms until a comparable performance with already existent RVNN libraries is reached. Instances of new layers and even learning algorithms unique to CVNN, can be easily added in future versions. Nevertheless, this library is of difficult use to inexperienced/unfamiliar users with ANN/Rust.

Results obtained by CVNN modulated in the library, typically outperform in loss and/or accuracy, have better generalization capabilities, take less epochs to train and offer more stability on local minima when compared to RVNN just as described in the literature (Chapter~\ref{sec:chap1_sota}). Nevertheless, due to the additional computations and memory usages involve in operations with complex numbers, CVNN experience more performance overhead and compared to RVNN. For this purposes, regularization, was not needed so far.

Regarding Future Work, in spite of this library being ready to be used for some applications or research, there is still a many of missing features. The next steps of development would be to implement the non-gradient based approach Multi-Valued Neuron to optimize a CVNN, more layer instances like 1D convolution and recurrent layers, and more activation functions like the ones described in Section~\ref{sec:CAF} with equal to greater potential as the core ones already implemented. Nevertheless, the most urgent steps to take is to optimize critical and core procedures in the library. This can be done, for instance, by the creation of thread pools when initiating the library to manage specific tasks associated with the forwarding and back-propagating a signal with concurrency, having an interface of predefined arrays to be stored in the stack memory (for quick memory access) or memory optimization in basic complex operations.

A very important study which could be conducted, would be a fair evaluation of runtime performance between RVNN and CVNN\footnote{At the time of writing this thesis, such study was not yet addressed in the literature to the best of the authors knowledge.}. Having Renplex fully optimized in every possible aspect, it would be of upmost importance to check if a CVNN is effectivelly faster than a RVNN since the former takes less epochs to train a model, despite the performance overhead introduced by complex computations. Also, comparing over-fitting thresholds between CVNN and RVNN would be equally interesting.

The library presented in this document is available and open for contributions on the GitHub address \href{https://github.com/Pxdr0-A/renplex.git}{Pxdr0-A/renplex.git}, and the author has the intention to give some further improvements on the library's core functionalities and documentation, where at the time, the latter can still be improved.
