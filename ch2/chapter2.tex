% Chapter 2

\chapter{State-of-the-Art}
\label{sec:chap1_sota}

For this \gls{SOTA}, only an overview of the literature is given as the primary objective is to build a library that modulates \gls{CVNN}. Inasmuch, the first step is to study relevant applications for \gls{CVNN}, which will give some insights on the data that our library should be able to handle. Additionally, it exists in the literature a vast number of approaches to develop a \gls{CVNN}, for such, the mathematical theory surrounding this topic will be addressed. Lastly, the already existent libraries are described to help define what the mentioned library adds to the body of knowledge to the research community.

The search was performed by extracting the most relevant studies for the development of the library using the string "complex-valued neural network" as the main keywords. It consisted in finding papers with popular applications that have the potential of being applied in the library, as well as, theory that can be used for defining the architecture of a \gls{CVNN}. Some extremely advanced or \gls{SOTA} procedures for modeling \gls{CVNN} were kept out of this work, since the intention is just to design a simple library that can scale for common \gls{CVNN} applications. Databases included in the search are the following: \href{https://scholar.google.com/}{Google Scholar} \href{https://ieeexplore.ieee.org/Xplore/home.jsp}{IEE Xplore}, \href{https://arxiv.org/}{arxiv.org}, \href{https://link.springer.com/}{Springer Link} and \href{https://www.sciencedirect.com/}{Science Direct}.

\section{Applications}
The majority of \gls{CVNN} applications come from the fact that the training data is written in the complex domain. Meanwhile, the rest relies on strategies to cast the data from the real domain into the complex domain. This section is subdivided into the various areas of application.

\subsection{Signal Processing}

Signal processing was the first application found for the topic at hand \parencite{hirose2012complex} and it is a vast field. For simplicity, this subsection is subdivide in applications related to Wireless Communications and Audio. Although, the nature of the data used is similar between some fields, there are some nuances to it.

\subsubsection{Wireless Communications}
Electromagnetic waves that constitute the signals present in wireless communications, are more mathematically accurate when represented by complex numbers. If a certain problem arises that can be solved by training a neural-based model, \gls{CVNN} can be considered. 

One of the focus on this field is using \gls{CVNN} for signal coherence \parencite{hirose2012cohersignal, wu2017singalcohercvnn}. It consists in providing the time signature, for instance, an electromagnetic signal to the inputs of the \gls{CVNN}, with the objective the reconstruct the coherent source signal, with as little Signal-to-Noise-Ratio as possible. \textcite{hirose2012cohersignal} describe a generalization for this problem and demonstrates that \gls{CVNN} can achieve better performance than its real counter-part, whereas \textcite{wu2017singalcohercvnn} goes deeply in the specific learning method to be applied for these cases. Current work will not contain the latter learning strategy, still it will address a similar task as in \textcite{hirose2012cohersignal}  at Chapter~\ref{cp:eval}, as it is one of the most fundamental tasks of CVNN.

Also, the field for developing signal equalizers is where \gls{CVNN} provides satisfactory solutions \parencite{uncini1999equalizerold, cheolwoo1998oldeuqalizer, hong2014equalizerhammer, liu2017equalizercvnn}. Such procedure, aims to mitigate the cross-modulation effects between the in-phase and quadrature-phase of the traveling signal, and it has been addressed since the late 90's \parencite{uncini1999equalizerold, cheolwoo1998oldeuqalizer}. This application goes in a similar fashion to the one described in the previous paragraph by \textcite{liu2017equalizercvnn}. 

Channel estimation is also an crucial aspect of wireless communications \parencite{murata2015cvnnforsignalchapter, yuan2019channel}. In this case, studies aim to classify a certain channel's characteristics. Herein, \gls{CVNN} receives the exchange signal in the complex domain. In the case of \parencite{yuan2019channel}, the authors implemented an auto-encoder architecture that a CVNN should be capable of supporting. Alongside channel estimation, \gls{CVNN} managed to surpass Real \gls{DL} models on the task of specific emitter identification \parencite{wang2021emmiteridcvnn}.

With the introduction of the 5G mobile network, a recent study shows an application \gls{CVNN} in the massive multiple-input multiple-output (MIMO) \parencite{tiba2023signalmimocvnn} . The motivation lies purely on the fact that current MIMO detection is done by \gls{RVNN}, which does an additional step of converting the complex data, that the signal naturally possesses, into real data. Not only there is loss of information, but also, unnecessary computational demand. Study shows that, a \gls{CVNN} can provide better performance in when compared to current detectors and reduce the computational cost \parencite{hirose2012cohersignal, tiba2023signalmimocvnn}. Some equally relevant studies precede this recent one, such as, \parencite{marseet2017firstmimo} where authors also use the architecture that was later used replicated by \parencite{yuan2019channel} in the channel estimation problem.

Generic studies on signal processing, have also been conducted over 20 years ago \parencite{kim2000envelope, kim2002perceptron}. The latter authors experimented with specific activation functions that improve the performance of \gls{CVNN} signal processing capabilities. Still, some more recent studies pick up from this point for further improvements and extensions \parencite{scardapane2018complex}.

\subsubsection{Audio}
Regarding audio analysis with \gls{CVNN}, some applications emerge in the field of speech recognition \parencite{hayakawa2018speech}. This application comes from the direct translation of incoming sound/wave signals from speech, which are already, by default, encoded in complex numbers with an amplitude and a phase. The speech signal is not analyzed in a recursive way, but a batch of the signal is analyzed to decode possible existent speech within it. A \gls{CVNN} does outperform a \gls{RVNN} in the task \parencite{hayakawa2018speech}. \textcite{hu2020dccrn}, also approaches the topic of analyzing speech. Nonetheless, in this specific case, the task is not to recognize but to enhance the signal. Similar to the channel estimation or specific emitter identification \parencite{wang2021emmiteridcvnn}, herein, a speech signal is introduced with noise and a \gls{CVNN} is tasked to enhance the quality of the signal, which is in fact able to achieve satisfactory results.

The above problem of enhancing an audio signal was also applied for the mp3 format in \parencite{al2012mp3enhance}. The objective was to recover an encoded signal, as close as possible to the unmodified, with the ideal architecture, one can get more suitable results when compared to an equivalent \gls{RVNN} \parencite{al2012mp3enhance}.

Strikingly, \gls{CVNN} also found an application in music by means of a classification task of retrieving meta-information about a song \parencite{kataoka1998music}, or memorizing a sequence of notes of a melody \parencite{kinouchi1996memorization}. In spite of \parencite{kataoka1998music} using Recurrent Neural Networks, the procedures is similar when compared to real numbers.

\subsection{Image Processing \& Computer Vision}

When it comes to Image Processing and \gls{CV} models, \gls{CVNN} also exhibit promising results, however, the way  data is handle can be different from the signal processing procedure.

In satellite imagery as in the example of f TerraSAR-X datasets, satellites can provide aside from a normal image, information about the polarization of the light received \parencite{gleich2018complex}. This in itself, can be represented in the complex domain together the classical image and feed onto a \gls{CVCNN}. Nevertheless in \parencite{gleich2018complex} the authors implement a \gls{CVCNN} with substantial results given the nature of the data.

Whereas, during data pre-processing stage for \gls{CVNN}, not all data has an explicit complex notation associated. \textcite{liu2014handgestures} created a model for hand gesture recognition, where the data is initially represented in pixels. The images are pre-processed with a \gls{CV} tool to get the main features out of the image, such as angles between fingers, length, etc. These coordinates are then written in the complex plane where the images can finally be processed by a \gls{CVNN} with a performance that matches current applications \parencite{liu2014handgestures}. Although not with the same detail, another work pre-dated this exact issue \parencite{hafiz2011handgesturecvnn}.

Other studies, circle around facial recognition on distinguishing between the two genders. Still features are extracted from an image with \gls{CV} tools. Despite that, \textcite{amilia2015face} performed a mapping that defines $ 1 + 0.5\imath $ as male and $ 0 + 0.5\imath $ as female instead of the real typical values of $0$ and $1$.

In \gls{DL} for Image Recognition, the typical \gls{CVNN} pipeline can be either converting an image's pixels to complex numbers with no imaginary component or to extract the features of the image and find some mapping to the complex plane, with these features \parencite{gu2018convcvnnvgg, chiheb2017deep}. One simple example can be for instance, getting the intensity of the transitions and respective angles with a Sobel Operator \parencite{sobel2014operator}, and the intensities and angles can be mapped to the absolute values and phases of the complex numbers respectively. Another option is to compress the features since a complex number can encode two numbers, the number of inputs can be resized to $ N/2 $ where $ N $ is the number of real inputs \parencite{gu2018convcvnnvgg}. \gls{CVNN} have also been employed in similar problems involving crowd counting \parencite{matlacz2018crowd}, where one can divide the complex components in features as stated previously or just give a default value to the imaginary part \parencite{chiheb2017deep, matlacz2018crowd}.

Furthermore, in the Image Processing realm, \gls{CVNN} can be used to reconstruct images that are blurred \parencite{aizenberg2008blurdetect, aizenberg2011blurbetter} similarly to signal reconstruction, but with mapping to real numbers and without requiring \gls{DL} models.

To wrap up this selection of applications, it is important to acknowledge that \gls{CVNN} are equally suitable outside classification or segmentation tasks, such as the ones reviewed up until now. A very recent study by \textcite{luo2024imagecompressalgocvnn} provides a solution for compressing images based on \gls{CVNN} with greater robustness against adversarial attacks when compared to \gls{RVNN}.

\subsection{Health}

\gls{CVNN} make an appearance in the health sector. Applications such as Electroencephalography (EEG) and Medical Resonance Imaging (MRI) where both signal can be divided in a real and an imaginary component. In the studies \parencite{du2023hybrid, zhang2017sleepsignal, peker2016eegsignal} the authors explore possible usages in EEG-related diagnosis. While \parencite{du2023hybrid} dive in a more generic study on trying to address if \gls{CVNN} are viable for EEG applications, \parencite{zhang2017sleepsignal, peker2016eegsignal} take a more pragmatic approach in trying to apply it to classifying sleep stages, and epilepsy diagnosis, respectively. In the former, there is the use of the complex convolution (will be later addressed) operation given the initial complex signal, where authors were able to match human experts performance \parencite{zhang2017sleepsignal}. In the latter, authors consider the relevance of the EEG as the gold standard for epilepsy diagnosis \parencite{pillai2006epi} to create a model that performs such evaluation, however, as opposed to the previous procedure, they do not rely on convolutional layers. Instead, the authors implement a Multi-Layered Perceptron \parencite{rumelhart1986} \gls{CVNN} with k-folds cross-validation to accurately be used for epilepsy diagnosis \parencite{peker2016eegsignal}.

In the MRI scenario, \parencite{cole2020analysis, cole2021analysis}, a cross-section of an image is defined in the complex plane with polar coordinates. The reconstruction process significantly reduces the amount of time patients need to remain still, so the authors explore Complex Convolutional Layers that perform this task. Being the input a matrix of complex values, \gls{CVNN}s were able to achieve higher quality of image reconstruction \parencite{cole2020analysis, cole2021analysis}. Still in the topic of MRI, the task of identifying tissue parameters, based on cross-sections, was also tackled with \gls{CVNN} in \parencite{virtue2017mribettercvnn} and outperformed \gls{RVNN} purposely designed for the task.

It is of high importance to note that operations such as the Fast Fourier Transform, can be implemented in \gls{CVNN}, which subsequently analyzes, for example, medical images regarding mammography for digital watermarking \textcite{olanrewaju2011watermarkmed}.

\subsection{Other Applications}
Although multiple cases indicate that CVNN typically surpasses RVNN in terms of performance metrics, CVNN is generally slower to train. As previously mentioned, the study by \textcite{zhang2021optical} develops for the first a computing ship specialized for complex computation involved in a complex-valued multi-layer perceptron. With this computing chip, the authors analyzed fundamental logic gates that adapted better to non-linearities when compared to RVNN. The IRIS \parencite{fisher1936} and MNIST \parencite{lecun1998mnist} dataset were also studied with slightly different strategies both achieving greater results than RVNN.

Interestingly enough, \gls{CVNN} found its way onto stock prediction \parencite{jia2018stockpredcvnn, wang2017cvnnstockprediction}. Stocks data is not written in the complex domain, therefore, authors described a method casting those values in a unitary complex value. This was by defining a phase based on the real data point, maximum value of the set, and minimum value of the set \parencite{wang2021emmiteridcvnn}. Both procedures also used different optimization algorithms for updating the weights, respectively,  Particle Swarm Optimization \parencite{eberhart1995particle} and Cuckoo search \parencite{yang2014cuckoo}.

There is also a Thesis that is worth mentioning on using and exploring Deep Complex-Valued Recurrent Neural Networks \parencite{monning2019deep}.

\section{Theory Behind \gls{CVNN}}

Multiple approaches have been taken into considerations when it comes to designing a \gls{CVNN} and many references have already dived deep into how should one structure a \gls{CVNN} depending on the task at hand. This section will be divided into two sub-sections. First about how the literature has approached the problem of defining a \gls{CAF} and how can the back-propagation be implemented in these networks. Some notes about the \gls{CBP} algorithm will be included based on the literature.

Related to a \gls{CVNN} library \parencite{barrachina2023theory}, provides also a detailed explanation on how to implement such networks code-wise, as well as, \parencite{abdalla2023newtheory} also provides a great and up-to-date summary on the many possibilities to design a \gls{CVNN}. For development purposes, former studies will be important for the library implementation.

\subsection{Complex Activation Function}
\label{sec:CAF}
The first steps onto the development of  a \gls{CVNN}, targeted \gls{CAF} \parencite{clarke1990definition, georgiou1992fullfunc}. There is an inherent problem of this neural networks that upon providing a complex argument to an activation function typically used in \gls{RVNN}\footnote{A standard example can be the Sigmoid function for instance}, one would observe that its derivatives are not contained/limited in the complex domain. This condition is important for the stability of the gradients. Next on this section, it will be addressed some \gls{CAF} studied in the literature that were implemented in the library.

The most standard example is the identity activation function or simply no activation first introduced by \textcite{widrow1975complex}. It is a function that is useful for drawing signals in the output layer for instance but it is prone to exploding/vanishing gradient \parencite{hirose2012complex}. The problem with the exploding/vanishing gradient can be tackled by normalizing $ z $ with the modulo function \parencite{amari1995information, hirose2012complex}, nevertheless it limits $ z $ to a circumference of radius $ 1 $ which is specially useful for the non-gradient based approach for learning \parencite{bassey2021survey}.

Although slightly unstable, the hyperbolic tangent function can also be used, being one of the first activation functions to be experimented with \parencite{kim2000fully}.

To tackle the problem of limited derivative,  \textcite{benvenuto1992firstback} suggested the split-type activation functions. This one consists in applying a well-known activation function use in RVNN like sigmoid \parencite{cox1958} to both the real and imaginary part separately (split-type A) or the amplitude and phase (split-type B) \parencite{abdalla2023newtheory}, regardless, such function does provide the much needed quick differentiation but it does not represent a fully \gls{CAF} with the possibility to incorporate correlations between real and imaginary component just like previous ones. By expanding this reasoning, one can define a set o split-functions based on RVNN activation functions making a very direct analogy between networks. 

\begin{equation}
	f_s(z) = f(\Real{z}) + if(\Imag{z}),
\end{equation}
where $ f(x) $ is a function with limited derivative in the real domain like the sigmoid, or some non-linear function like ReLU \parencite{glorot2011deep}. Within the same context, one can have complex non-linear/parametric functions in the complex domain taking in consideration the phase, which is that case for the zReLU function \parencite{guberman2016complex}.

\begin{equation}
	f(z) = \begin{cases}
		z \ \text{if} \ \text{arg}(z) \in \left[ 0, \frac{\pi}{2} \right]  \\
		\\
		0 \ \text{otherwise} \\ 
	\end{cases}
\end{equation}

All these functions were later referred to as non-analytical \parencite{scardapane2020newactfuncs}, and those that involved the necessity to compute absolute values, also fall in the same category.

Another non-analytical \gls{CAF} worth noting is the Cardioid Function \parencite{virtue2017mribettercvnn}. It is easily differentiable to help in the \gls{CBP} and carries a simple expression of basic computation, given by,

\begin{equation}
	f(z) = \dfrac{1}{2} \left( 1 + \cos(\arg(z)) \right) z. 
	\label{eq:cardioidfunc}
\end{equation}

The above mentioned \gls{CAF}, describes the main core types of activation in the complex domain that the library will implement. Nonetheless, in the next paragraph will be mentioned some \gls{CAF} that have proved to provide comparable or better performance but were not yet implemented in the library. 

Although used in some specific contexts, the modReLU function is a variant of the ReLU function in the complex domain first introduced by \textcite{arjovsky2016unitary}. This function can be prone to training since it possesses a threshold that needs to be defined unlike the split-ReLU function for instance.

\begin{equation}
	f(z) = \text{ReLU}(\norm{z} + b) \frac{z}{\norm{z}},
\end{equation}
where $ b $ is the threshold.

Authors in \parencite{scardapane2020newactfuncs} describe the usage of these so called \gls{KAF} that when incorporated in a \gls{CVNN}, show better  results when compared to \gls{RVNN} for the standard dataset MINIST \parencite{scardapane2020newactfuncs, lecun1998mnist}. Although they are not used in this library, it is important to mention their existence that they provide good performance on CVNN models and could be implemented in the library. To avoid getting too technical in this \gls{SOTA}, broad terms, \gls{KAF} are constructed with a kernel that is easily differentiable and limited in a weighted sum along a grid. Typical  \gls{KAF} would read as,

\begin{equation}
	g(z) = \sum_{n=1}^{D}\sum_{m=1}^{D} \alpha_{n,m} \kappa_{\mathcal{C}}\left( z, d_n + \imath d_m \right),
\end{equation}
with $D$ being the dimension of the grid, $\kappa_{\mathcal{C}}$ the kernel (can be for instance a Gaussian function), ${d_n, d_m}$ are parameters of the grid and $\alpha_{n,m}$ the mixing parameters \parencite{hofmann2008kernel, liu2011kernel}.

\subsection{Complex Learning Procedure}

In later sections, a more detailed walk-through on the \gls{CBP} algorithm, to be used in this Master Thesis, will be provided as well as other possible learning alternatives. This section, briefly discusses some of the already available options in the literature.

\subsubsection{Core Procedures}

One can divide into two main topics, gradient-based and a non-gradient based learning \parencite{bassey2021survey, abdalla2023newtheory}.

With the gradient approach, the objective is, as known from the classical neural networks, to compute the gradient of the cost/loss function. The loss can be calculated in the complex domain for a single training sample like so,

\begin{equation}
	\mathcal{L} = \sum_n \left| a_n - t_n  \right| ^2,
\end{equation}
where $a_n \in \set{C}$ represents the activation on the last layer per unit $n$ and $t_n \in \set{C}$ the desired output from the training sample.

The back-propagation can be performed with almost the same procedure as in a \gls{RVNN}, however, one can either analyze the adjustment to the weights at the real and imaginary level individually (split \gls{CBP}) \parencite{benvenuto1992firstback} or at the weight as an entire complex number (full \gls{CBP}) since they obey the same differential properties \parencite{kim2000fully, li2006backpropold, hirose2012complex, bassey2021survey, abdalla2023newtheory}. Moreover, one must recognize the foundation required to reach the complex gradient of the loss within the Wirtinger Calculus \parencite{wirtinger1927formalen}. From surveys \parencite{bassey2021survey, lee2022survey}, one can see that the full \gls{CBP} is more commonly used and the split \gls{CBP} is gradually becoming obsolete due to not considering the correlations between real and imaginary parts individually.

One thing that CVNN bring of new when compared to RVNN is that one can apply a non-gradient based approach for training the network. The error correction occurs at the phase level being the reason why the modulo activation function is so important. The correction is defined and discussed in \parencite{aizenberg1973multivalued} and as stated, does not involve computing any derivatives. For a simple feed-forward \gls{CVNN}, the expression for the error correction can be found in \parencite{abdalla2023newtheory} and its interpretation is that the error is corrected at the neuron level (individually). As the error begins at the output layer, in this approach, it still needs to be propagated backwards. The implementation of this algorithm in the library is incomplete.

\subsubsection{Further Considerations}

The mentioned core procedures already underwent some improvements, or simply new additional  methods have been adopted. Among them, \textcite{liu2017efficientweightupdate} proposed a more efficient algorithm for updating the weights of a \gls{CVNN}. It was achieved by separating the training method of the input layer, from the output layer. In the latter, the update to the output neuron's weights is first calculated with the least squares method. Getting this result, error is propagated with \gls{CBP} using Gradient Decent to  the input's weights. Authors reported a better convergence and generalization capabilities \parencite{liu2017efficientweightupdate}.  

Modifications to the learning algorithm with momentum optimization or even the Adam optimizer, also provide same benefits as in the \gls{RVNN} \parencite{kotsovsky2020new, kingma2014adam}. The same applies for adaptive complex-valued step sizes in gradient descent, however, this procedure can only be applied to fully \gls{CVNN} \parencite{zhao2023ortogradientcvnn, weijing2024gradientdescentcvnn}.

Convolution operation is well defined in the complex plain and, just like in the \gls{RVNN}, calculations can be parallelized. In spite of the slight increase in complexity, \gls{CVCNN} out-perform a \gls{CNN} \parencite{guberman2016complex, chatterjee2022cvnncomparison}.

Last but not least, if designed properly, \gls{CVNN}  have been shown to be more robust than a its real counter-parts, specially in signal processing-related applications \parencite{neacsu2022protectcvnn}.

\section{Exploration of Existent Libraries}
As stated by \parencite{bassey2021survey}, there is the need for libraries targeted for complex-valued computations such as in \gls{CVNN}, hence the objective of this Master Thesis. Very few full-fledged libraries or toolboxes can be found in the literature that modulate \gls{CVNN}. To the best of the authors knowledge, only the following references were found: \parencite{trabelsi2017tool, cruz4252610rosenpy, peker2015tool, j_agustin_barrachina_2022_7303587, dramsch2019complex, barrachina2023theory}.

Within this group one finds, in its majority, libraries that are built on top of an already existent machine learning framework meant for RVNN modeling like \href{https://keras.io/}{Keras}. \parencite{trabelsi2017tool, cruz4252610rosenpy, j_agustin_barrachina_2022_7303587, dramsch2019complex, barrachina2023theory} all use \href{https://keras.io/}{Keras} / \href{https://www.tensorflow.org/}{TensorFlow} as a back-end which may limit the amount of available operations and architectures, further performance optimizations and efficient memory management, given the unique approaches, as observed from the current \gls{SOTA}, that \gls{CVNN} may require. Additionally, all of these tools have not been updated since 2 years and were developed typically for a one-time-usage.

\textcite{peker2015tool} has also developed a tool for signal process, nonetheless, from \parencite{peker2016eegsignal} it suggests that was built for this specific usage.

\section{Data Protection \& Ethical Aspects}
Given the nature of the objective of this Thesis, it is not going to involve managing data of confidential or bio-metric related that might invoke any privacy policy. A synthetic signal reconstruction dataset will be produced and used for testing CVNN as well as and the MNIST dataset \parencite{lecun1998mnist} in two applications.

Current state of the library is not yet capable of training large scale models,
or even generative models in the dangerous category of the \textcite{euai2021}.